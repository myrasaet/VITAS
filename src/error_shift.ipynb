{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants import base_path, model_list\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import ast\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_df_valid = pd.read_csv(f\"{base_path}\\\\input\\\\release_validate_patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{base_path}\\\\input\\\\release_evidences.json\") as f:\n",
    "  evidences = json.load(f)\n",
    "evidences_list = []\n",
    "evidences_dict = {}\n",
    "evidences_en_to_code = {}\n",
    "for e in evidences.keys():\n",
    "  # only binary symptoms and no antecedents\n",
    "  if (not evidences[e][\"possible-values\"]):\n",
    "    evidences_list.append(e)\n",
    "    evidences_dict[e] = evidences[e][\"question_en\"]\n",
    "    evidences_en_to_code[evidences[e][\"question_en\"]] = e\n",
    "evidences_code_to_en = evidences_dict\n",
    "evidences_list_en = list(evidences_en_to_code.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{base_path}\\\\input\\\\release_conditions.json\") as f:\n",
    "  disease_dict = json.load(f)\n",
    "disease_list = list(disease_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misses(degraded_df):\n",
    "    missed_evidences_per_disease = {}\n",
    "    for disease in disease_list:\n",
    "        disease_df = degraded_df[degraded_df[\"PATHOLOGY\"]==disease]\n",
    "\n",
    "        # here, we want to count the most missed evidence per disease\n",
    "        missed_evidences = []\n",
    "        for e in disease_df[\"missed_evidence\"]:\n",
    "            e = ast.literal_eval(e)\n",
    "            if e:\n",
    "                missed_evidences.extend(e)\n",
    "        missed_evidences_dict = dict(Counter(missed_evidences).most_common(10))\n",
    "        missed_evidences_dict = {evidences_code_to_en[k]:missed_evidences_dict[k] for k in missed_evidences_dict}\n",
    "\n",
    "        # hypothesis: the initial evidence is not very specific, hence the degrade in predictions\n",
    "        initial_evidences_dict = dict(Counter(list(disease_df[\"INITIAL_EVIDENCE\"])).most_common(10))\n",
    "        initial_evidences_dict = {\n",
    "            evidences_code_to_en[k]: initial_evidences_dict[k]\n",
    "            for k in initial_evidences_dict}\n",
    "        missed_evidences_per_disease[disease] = {\n",
    "            \"top_missed_evidences\": missed_evidences_dict,\n",
    "            \"initial_evidences_count\": initial_evidences_dict\n",
    "        }\n",
    "    return missed_evidences_per_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df_questionnaire = pd.read_csv(f\"{base_path}\\\\output\\\\error_analysis_questionnaire\\\\questionnaire_df.csv\", index_col=False).drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_outputs(model_name):\n",
    "    print(f\"Analyzing {model_name}...\")\n",
    "    valid_df_pred = pd.read_csv(f\"{base_path}\\\\output\\\\error_analysis\\\\{model_name}\\\\validation_df_all_patients.csv\", index_col=False).drop([\"Unnamed: 0\"], axis=1)\n",
    "    valid_df_pred = valid_df_pred[[\"PATHOLOGY\", \"predicted_diagnosis\", \"is_matched\"]]\n",
    "    valid_df_pred_questionnaire = pd.read_csv(f\"{base_path}\\\\output\\\\error_analysis_questionnaire\\\\{model_name}\\\\validation_df_all_patients_questionnaire.csv\", index_col=False).drop([\"Unnamed: 0\"], axis=1)\n",
    "    valid_df_pred_questionnaire = valid_df_pred_questionnaire[[\"predicted_diagnosis\", \"is_matched\"]]\n",
    "    match_df = valid_df_pred.join(valid_df_pred_questionnaire.add_suffix(\"_questionnaire\"))\n",
    "    degraded_df = match_df[(match_df['is_matched']==True) & (match_df['is_matched_questionnaire']==False)]\n",
    "    save_path = f\"{base_path}\\\\output\\\\error_analysis_questionnaire\\\\{model_name}\"\n",
    "    degraded_df[\"PATHOLOGY\"].value_counts().sort_values().plot.barh(figsize=(6, 8))\n",
    "    plt.title(f\"Pathology count - Degraded Predictions\\n{model_name}\")\n",
    "    plt.savefig(f'{save_path}\\\\pathology_freq_degraded.jpg', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    degraded_df = degraded_df.join(valid_df_questionnaire)\n",
    "    missed_evidences_per_disease = get_misses(degraded_df)\n",
    "    with open(f\"{save_path}\\\\missed_evidences.json\", \"w\") as outfile: \n",
    "        json.dump(missed_evidences_per_disease, outfile, indent=True)\n",
    "    # most common initial evidence for the degrades\n",
    "    degraded_df.INITIAL_EVIDENCE.map(evidences_code_to_en).value_counts().sort_values().tail(10).plot.barh()\n",
    "    plt.title(f\"Initial Evidence Count - Degraded Predictions\\n{model_name}\")\n",
    "    plt.savefig(f'{save_path}\\\\initial_evidence_freq_degraded.jpg', bbox_inches='tight')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing decision_tree...\n",
      "Analyzing random_forest...\n",
      "Analyzing gradient_boost...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_name in model_list[\"tree-based\"]:\n",
    "    analyze_outputs(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing logistic_regression...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_outputs(\"logistic_regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitas-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
